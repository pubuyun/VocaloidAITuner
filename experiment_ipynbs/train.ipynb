{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training data for rnn model from ./trainfiles\\\n",
    "each csv file is a sequence of data\\\n",
    "each row is a data point\\\n",
    "each column is a feature\\\n",
    "first three columns (Dynamic, rPitch, Velocity) are the targets\\\n",
    "the last two columns (Pitch, Lyric) are the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydict = [\"a\",\"ai\",\"an\",\"ang\",\"ao\",\"shi\",\"liu\",\"ge\",\"ba\",\"bai\",\"ban\",\"bang\",\"bao\",\"bei\",\"ben\",\"beng\",\"bi\",\"bian\",\"biao\",\"bie\",\"bin\",\"bing\",\"po\",\"bo\",\"bu\",\"san\",\"si\",\"ca\",\"cai\",\"can\",\"cang\",\"cao\",\"ce\",\"cen\",\"ceng\",\"cha\",\"chai\",\"chan\",\"chang\",\"zhang\",\"chao\",\"che\",\"chen\",\"cheng\",\"chi\",\"chong\",\"chou\",\"chu\",\"chuai\",\"chuan\",\"chuang\",\"chui\",\"chun\",\"chuo\",\"ci\",\"cong\",\"cou\",\"cu\",\"cuan\",\"cui\",\"cun\",\"cuo\",\"er\",\"da\",\"dai\",\"dan\",\"dang\",\"dao\",\"de\",\"deng\",\"di\",\"dian\",\"diao\",\"die\",\"ding\",\"diu\",\"dong\",\"dou\",\"du\",\"duan\",\"dui\",\"dun\",\"duo\",\"e\",\"en\",\"jiu\",\"fa\",\"fan\",\"fang\",\"fei\",\"fen\",\"feng\",\"fu\",\"fou\",\"ga\",\"gai\",\"gan\",\"gang\",\"gao\",\"gei\",\"gen\",\"geng\",\"gong\",\"gou\",\"gu\",\"gua\",\"guai\",\"guan\",\"guang\",\"gui\",\"gun\",\"guo\",\"ha\",\"hai\",\"han\",\"xing\",\"hang\",\"hao\",\"he\",\"hei\",\"hen\",\"heng\",\"hong\",\"hou\",\"hu\",\"hua\",\"huai\",\"huan\",\"huang\",\"hui\",\"hun\",\"huo\",\"ji\",\"jia\",\"jian\",\"jiang\",\"jiao\",\"jie\",\"jin\",\"jing\",\"qing\",\"jiong\",\"ju\",\"juan\",\"jue\",\"jun\",\"ka\",\"kai\",\"kan\",\"kang\",\"kao\",\"ke\",\"ken\",\"keng\",\"kong\",\"kou\",\"ku\",\"kua\",\"kuai\",\"kuan\",\"kuang\",\"kui\",\"kun\",\"kuo\",\"wu\",\"la\",\"lai\",\"lan\",\"lang\",\"lao\",\"le\",\"lei\",\"leng\",\"li\",\"lia\",\"lian\",\"liang\",\"liao\",\"lie\",\"lin\",\"ling\",\"long\",\"lou\",\"lu\",\"lv\",\"luan\",\"lve\",\"lun\",\"luo\",\"ma\",\"mai\",\"man\",\"mang\",\"mao\",\"me\",\"mei\",\"men\",\"meng\",\"mi\",\"mian\",\"miao\",\"mie\",\"min\",\"ming\",\"mo\",\"mou\",\"mu\",\"na\",\"nai\",\"nan\",\"nang\",\"nao\",\"nei\",\"nen\",\"neng\",\"ni\",\"nian\",\"niang\",\"niao\",\"nie\",\"nin\",\"ning\",\"niu\",\"nong\",\"nu\",\"nv\",\"nuan\",\"nve\",\"nuo\",\"yi\",\"ou\",\"qi\",\"pa\",\"pai\",\"pan\",\"pang\",\"pao\",\"pei\",\"pen\",\"peng\",\"pi\",\"pian\",\"piao\",\"pie\",\"pin\",\"ping\",\"pou\",\"pu\",\"qia\",\"qian\",\"qiang\",\"qiao\",\"qin\",\"qiong\",\"qiu\",\"qu\",\"quan\",\"que\",\"qun\",\"ran\",\"rang\",\"rao\",\"re\",\"ren\",\"reng\",\"ri\",\"rong\",\"rou\",\"ru\",\"ruan\",\"rui\",\"run\",\"ruo\",\"sa\",\"sai\",\"sang\",\"sao\",\"se\",\"sen\",\"seng\",\"sha\",\"shai\",\"shan\",\"shang\",\"shao\",\"she\",\"shen\",\"sheng\",\"shou\",\"shu\",\"shua\",\"shuai\",\"shuan\",\"shuang\",\"shui\",\"shun\",\"shuo\",\"song\",\"sou\",\"su\",\"suan\",\"sui\",\"sun\",\"suo\",\"ta\",\"tai\",\"tan\",\"tang\",\"tao\",\"te\",\"teng\",\"ti\",\"tian\",\"tiao\",\"tie\",\"ting\",\"tong\",\"tou\",\"tu\",\"tuan\",\"tui\",\"tun\",\"wa\",\"wai\",\"wan\",\"wang\",\"wei\",\"wen\",\"weng\",\"wo\",\"xi\",\"xia\",\"xian\",\"xiang\",\"xiao\",\"xie\",\"xin\",\"xiong\",\"xiu\",\"xu\",\"xuan\",\"xue\",\"xun\",\"ya\",\"yan\",\"yang\",\"yao\",\"ye\",\"yin\",\"ying\",\"you\",\"yong\",\"yu\",\"yuan\",\"yue\",\"yun\",\"za\",\"zai\",\"zan\",\"zang\",\"zao\",\"ze\",\"zei\",\"zen\",\"zeng\",\"zha\",\"zhai\",\"zhan\",\"zhao\",\"zhe\",\"zhen\",\"zheng\",\"zhi\",\"zhong\",\"zhou\",\"zhu\",\"zhua\",\"zhuai\",\"zhuan\",\"zhuang\",\"zhui\",\"zhun\",\"zhuo\",\"zi\",\"zong\",\"zou\",\"zu\",\"zuan\",\"zui\",\"zun\",\"zuo\",\"shei\",\"chua\",\"dei\",\"den\",\"ne\",\"dia\",\"fo\",\"lo\",\"miu\",\"nou\",\"o\",\"qie\",\"tuo\",\"zhei\",\"ei\"]\n",
    "class RNNDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = os.listdir(data_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        df = pd.read_csv(file_path)\n",
    "        # get the inputs and targets\n",
    "        inputs = df.iloc[:,3:]\n",
    "        targets = df.iloc[:,:3]\n",
    "        # covert to tensor\n",
    "        inputs = torch.tensor(inputs.values, dtype=torch.float)\n",
    "        targets = torch.tensor(targets.values, dtype=torch.float)\n",
    "        return inputs, targets\n",
    "train_dataset = RNNDataset('trainfiles')\n",
    "# create a trainloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# input_size : [n, 1 + len(ydict)]\n",
    "# output_size : [n, 3]\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # Packing\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM\n",
    "        packed_output, _ = self.lstm(x_packed)\n",
    "        \n",
    "        # Unpacking\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(output)\n",
    "        return out\n",
    "\n",
    "# Define your model\n",
    "input_size = 1+len(ydict)  # Input size depends on the length of ydict\n",
    "hidden_size = 50  # You can define it as per your requirement\n",
    "num_layers = 2  # You can define it as per your requirement\n",
    "output_size = 3  # Output size is 3 as per your requirement\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "torch.Size([1, 121800, 407]) 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 36\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(sequences\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mlen\u001b[39m(lengths))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     16\u001b[0m x_packed \u001b[38;5;241m=\u001b[39m pack_padded_sequence(x, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# LSTM\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m packed_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_packed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Unpacking\u001b[39;00m\n\u001b[0;32m     22\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:777\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    774\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 777\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    780\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Get data\n",
    "        sequences, labels = batch\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        print(sequences.shape, len(lengths))\n",
    "        # Forward pass\n",
    "        outputs = model(sequences, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "# load the model\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model = torch.load(\"model.pth\")\n",
    "    print(\"Model loaded\")\n",
    "# Define criterion and optimizer\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 512\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test_dataset = RNNDataset('testfiles')\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "for batch in test_loader:\n",
    "    sequences, labels = batch\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    sequences = sequences.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(sequences, lengths)\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(\"test loss: \", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
